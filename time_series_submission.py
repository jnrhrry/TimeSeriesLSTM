# -*- coding: utf-8 -*-
"""Time Series Submission.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1XdTUXwgi9aPNuXArfCxSIrYLaRR_X7JO

Import Necessary Library
"""

import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
import tensorflow as tf
from keras.layers import Dense, LSTM
import keras
import keras.callbacks
from keras.callbacks import TensorBoard

"""Import Dataset"""

df = pd.read_csv('heart_rate_throughout_a_day.csv')
df

df.info()

"""Converting to float32"""

df['value'] = df['value'].astype(np.float32)

df.info()

df.isnull().sum()

time = df['time'].values
value = df['value'].values

time = np.array(time)
value = np.array(value)

plt.figure(figsize = (15,5))
plt.plot(time, value)
plt.title('Heart Rate', fontsize=20);

split_time = int(len(time)*0.8)
time_train = time[:split_time]
value_train = value[:split_time]
time_val = time[split_time:]
value_val = value[split_time:]

window_size = 30
batch_size = 32
shuffle_buffer_size = 1000

def windowed_dataset(series, window_size, batch_size, shuffle_buffer):
  series = tf.expand_dims(series, axis = -1)
  ds = tf.data.Dataset.from_tensor_slices(series)
  ds = ds.window(window_size + 1, shift = 1, drop_remainder = True)
  ds = ds.flat_map(lambda w: w.batch(window_size + 1))
  ds = ds.shuffle(shuffle_buffer)
  ds = ds.map(lambda w: (w[:-1], w[1:]))
  return ds.batch(batch_size).prefetch(1)

train_set = windowed_dataset(value_train, window_size = 60, batch_size = 200, shuffle_buffer= 1000)

val_set = windowed_dataset(value_val, window_size = 60, batch_size = 200, shuffle_buffer= 1000)

model = tf.keras.models.Sequential([
  tf.keras.layers.Bidirectional(LSTM(60, return_sequences=True)),
  tf.keras.layers.Bidirectional(LSTM(60)),
  tf.keras.layers.Dense(30, activation="relu"),
  tf.keras.layers.Dropout(0.7),
  tf.keras.layers.Dense(10, activation="relu"),
  tf.keras.layers.Dense(10, activation="relu"),
  tf.keras.layers.Dense(1),
])

class realCallback(tf.keras.callbacks.Callback):
  def on_epoch_end(self, epoch, logs={}):
    if(logs.get('mae')<10):
      print("\nMAE model less than 10% data scale.")
      self.model.stop_training = True

callbacks = realCallback()

optimizer = tf.keras.optimizers.SGD(lr=1.0000e-05, momentum=0.9)
model.compile(loss=tf.keras.losses.Huber(),
              optimizer=optimizer,
              metrics=["mae"])
history = model.fit(train_set, epochs=500, callbacks=[callbacks], validation_data= val_set)

def plot_graphs(history, string):
  plt.plot(history.history[string])
  plt.plot(history.history['val_'+string])
  plt.xlabel("Epochs")
  plt.ylabel(string)
  plt.legend([string, 'val_'+string])
  plt.title('MAE')
  plt.show()
  
plot_graphs(history, "mae")
plot_graphs(history, "loss")